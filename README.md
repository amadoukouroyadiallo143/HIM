# HIM - Hyper-Introspective Model (7M Parameters)

**Un mod√®le Transformer ultra-compact avec m√©canismes cognitifs avanc√©s inspir√©s du cerveau humain**

---

## üéØ Vue d'Ensemble

HIM est un mod√®le de langage exp√©rimental de **7.16 millions de param√®tres** qui impl√©mente quatre m√©canismes cognitifs sophistiqu√©s tout en maintenant une efficacit√© param√©trique extr√™me. C'est une r√©duction de **99.66%** par rapport √† l'architecture originale de 2.1 milliards de param√®tres.

### Innovations Cl√©s

- **Architecture Hybride**: Transformer optimis√© + m√©canismes cognitifs l√©gers
- **Grouped-Query Attention (GQA)**: Inspir√© de Llama-2/Mistral
- **SwiGLU Activation**: Comme PaLM/Llama
- **M√©moire de Travail**: DNC simplifi√©e avec projections low-rank
- **Raisonnement Explicite**: Buffer partag√© pour Chain-of-Thought
- **Apprentissage Multi-Vitesse**: Chemins rapide/lent (Syst√®me 1/2)
- **Entra√Ænement Avanc√©**: Learning rates multi-vitesse, pertes auxiliaires cognitives

---

## üìä Sp√©cifications du Mod√®le

| Composant | Configuration | Param√®tres | % |
|-----------|---------------|------------|---|
| **Vocabulaire** | BPE (30K tokens) | - | - |
| **Embedding** | 30,000 √ó 160 | 5,120,000 | 68.5% |
| **Positional Encoding** | Sinusoidal (buffer) | 0 | 0% |
| **Transformer Encoder** | 10 couches | 2,644,800 | 35.4% |
| ‚îî‚îÄ Attention | GQA (8Q/2KV heads) | 640,000 | 9.1% |
| ‚îî‚îÄ Feed-Forward | SwiGLU (320 hidden) | 1,024,000 | 14.5% |
| ‚îî‚îÄ Memory Interface | Low-rank (rank 16) | 51,200 | 0.7% |
| ‚îî‚îÄ Multi-Speed Gates | Routage dynamique | 1,600 | 0.02% |
| **Reasoning Module** | 64 tokens (partag√©) | 30,720 | 0.41% |
| **Memory System** | 16 slots (partag√©) | 12,800 | 0.17% |
| **Decoder** | Cross-attention + projection | 74,560 | 0.99% |
| **TOTAL** | | **7,157,680** | **100%** |

### Comparaison avec le Mod√®le Original

| M√©trique | Avant | Apr√®s | Am√©lioration |
|----------|-------|-------|--------------|
| **Param√®tres** | 2.1 milliards | 7.16M | **-99.66%** |
| **Embedding dim** | 1024 | 160 | -84.4% |
| **Layers** | 24 | 10 | -58.3% |
| **FFN dim** | 4096 | 320 | -92.2% |
| **Vocab** | 50,257 | 30,000 | -40% |
| **Attention** | MHA (16 heads) | GQA (8Q/2KV) | -50% heads KV |
| **VRAM Training** | ~24 GB | ~2 GB | -91.7% |
| **Vitesse Inf√©rence** | ~50 tok/s | >500 tok/s | **+900%** |

---

## üß† Les 4 M√©canismes Cognitifs

### 1. M√©moire de Travail Externe (DNC Simplifi√©)
```python
class UltraLightweightMemory:
    - 16 slots de m√©moire
    - Projections low-rank (rank 16)
    - Lecture/√©criture avec attention
    - M√©canisme erase-then-add (style DNC)
    - Co√ªt: 12,800 param√®tres (0.17%)
```
**B√©n√©fice**: Stockage contextuel persistant, apprentissage few-shot

### 2. Attention Sp√©cialis√©e (GQA)
```python
class GroupedQueryAttention:
    - 8 t√™tes Query
    - 2 t√™tes Key-Value (partag√©es 4:1)
    - R√©duction 75% du cache KV
    - Prouv√© dans Llama-2, Mistral, Gemma
    - Co√ªt: 640K param√®tres
```
**B√©n√©fice**: Efficacit√© param√©trique sans perte de qualit√©

### 3. Apprentissage Multi-Vitesse (Fast/Slow)
```python
class MultiSpeedLayer:
    - Chemin rapide: Identit√© (instantan√©)
    - Chemin lent: SwiGLU FFN (transformation compl√®te)
    - Routage dynamique appris (gate)
    - Co√ªt: 160 param√®tres par couche
```
**B√©n√©fice**: Cognition adaptative Syst√®me 1 (intuitif) / Syst√®me 2 (analytique)

### 4. Raisonnement Explicite (Chain-of-Thought)
```python
class SharedReasoningModule:
    - Buffer de 64 tokens partag√©
    - Raffinement tous les 3 layers (3, 6, 9)
    - Moving average pour mise √† jour
    - Co√ªt: 30,720 param√®tres (partag√©)
```
**B√©n√©fice**: Raisonnement multi-√©tapes, "scratchpad" mental

---

## üöÄ Installation et Utilisation

### Installation

```bash
# Cloner le repo
git clone https://github.com/votre-repo/HIM.git
cd HIM

# Cr√©er environnement virtuel
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Installer d√©pendances
pip install -r requirements.txt
```

### Entra√Ænement du Tokenizer (REQUIS EN PREMIER)

```bash
# Entra√Æner le tokenizer BPE avec 30K tokens
python train_tokenizer.py --vocab_size 30000 --dataset_name wikitext --config_name wikitext-103-v1

# Les fichiers seront sauvegard√©s dans: model_artifacts/tokenizer/
```

### Entra√Ænement du Mod√®le

```bash
# Configuration de base (recommand√©e)
python main.py \
    --task_type wikitext \
    --num_epochs 30 \
    --batch_size 8 \
    --accumulation_steps 4 \
    --learning_rate 1e-3 \
    --fp16 \
    --use_cognitive_losses \
    --label_smoothing 0.1

# Configuration compl√®te avec toutes les options
python main.py \
    --task_type wikitext \
    --embed_dim 160 \
    --num_heads 8 \
    --num_kv_heads 2 \
    --num_encoder_layers 10 \
    --dim_feedforward 320 \
    --mem_slots 16 \
    --mem_rank 16 \
    --reasoning_tokens 64 \
    --num_epochs 30 \
    --batch_size 8 \
    --accumulation_steps 4 \
    --learning_rate 1e-3 \
    --grad_clip_value 1.0 \
    --dropout 0.1 \
    --label_smoothing 0.1 \
    --fp16 \
    --use_cognitive_losses \
    --smart_training \
    --skip_threshold 0.5 \
    --pruning_amount 0.2 \
    --fine_tune_epochs 2 \
    --save_steps 1000
```

### Inf√©rence

```bash
# Mode interactif
python inference.py --mode interactive --model_path model_artifacts/nl_direct_response_model_best.pth

# Mode batch (JSON)
python inference.py --mode batch --input_file samples.json --output_file results.json
```

---

## üìà M√©triques d'Entra√Ænement

### M√©triques Standards
- **Perplexit√©**: Mesure de la qualit√© du mod√®le de langage
- **BERTScore**: Similarit√© s√©mantique avec r√©f√©rences
- **Distinct-1/2**: Diversit√© lexicale (pr√©vient r√©p√©tition)
- **BLEU**: Qualit√© de g√©n√©ration

### M√©triques Cognitives (NOUVEAU)
- **Memory Utilization**: % de slots m√©moire activement utilis√©s
- **Reasoning Buffer Evolution**: Changements dans le buffer de raisonnement
- **Routing Statistics**: Pr√©f√©rence slow vs fast par couche

### Pertes Auxiliaires Cognitives
```python
Total Loss = LM Loss + Œ±¬∑Memory Loss + Œ≤¬∑Reasoning Loss + Œ≥¬∑Routing Loss

o√π:
  - Memory Loss: Encourage l'utilisation s√©lective (sparsit√©)
  - Reasoning Loss: Pr√©vient le mode collapse (diversit√©)
  - Routing Loss: √âquilibre slow/fast (√©vite dominance d'un chemin)

Coefficients par d√©faut: Œ±=0.01, Œ≤=0.01, Œ≥=0.01
```

---

## üîß Architecture Technique

### Flux de Donn√©es

```
Input [B, 512]
  ‚Üì
Token Embedding (30K vocab) + Positional Encoding
  ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 10√ó EfficientTransformerBlock                   ‚îÇ
‚îÇ                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
‚îÇ  ‚îÇ Pre-LayerNorm                        ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ Grouped-Query Attention (8Q, 2KV)    ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ Residual + Dropout                   ‚îÇ       ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§       ‚îÇ
‚îÇ  ‚îÇ Pre-LayerNorm                        ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ Multi-Speed FFN (SwiGLU)             ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ   ‚îú‚îÄ Slow path (transformation)      ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ   ‚îú‚îÄ Fast path (identit√©)            ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ   ‚îî‚îÄ Gate (routage dynamique)        ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ Residual + Dropout                   ‚îÇ       ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§       ‚îÇ
‚îÇ  ‚îÇ Memory Interface (low-rank)          ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ   ‚îú‚îÄ Read from shared memory         ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ   ‚îî‚îÄ Write to shared memory          ‚îÇ       ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§       ‚îÇ
‚îÇ  ‚îÇ Reasoning Refinement (tous les 3)    ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ   ‚îî‚îÄ Update shared reasoning buffer  ‚îÇ       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
  ‚Üì
Decoder Query (learnable 64 tokens)
  ‚Üì
Cross-Attention avec encoder output
  ‚Üì
Projection to Vocabulary (weight-tied)
  ‚Üì
Logits [B, 64, 30K]
```

### Optimizer Multi-Vitesse

```python
Parameter Groups:
  1. Slow params (attention, FFN slow):    LR = base_lr √ó 0.3
  2. Fast params (gates, routing):         LR = base_lr √ó 1.5
  3. Cognitive params (memory, reasoning): LR = base_lr √ó 0.5
  4. Standard params (autres):             LR = base_lr

Scheduler: OneCycleLR
  - 10% warmup
  - Cosine annealing
  - Division factor: 25 (initial), 1000 (final)
```

---

## üìö Structure du Projet

```
HIM/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ components.py          # Tous les composants architecturaux
‚îÇ   ‚îú‚îÄ‚îÄ model.py               # Classe NLDirectResponse principale
‚îÇ   ‚îú‚îÄ‚îÄ dataset.py             # Chargement WikiText
‚îÇ   ‚îú‚îÄ‚îÄ train.py               # Entra√Ænement avanc√© avec pertes cognitives
‚îÇ   ‚îî‚îÄ‚îÄ optimizer.py           # Lion optimizer
‚îú‚îÄ‚îÄ main.py                    # Point d'entr√©e entra√Ænement
‚îú‚îÄ‚îÄ inference.py               # Script d'inf√©rence
‚îú‚îÄ‚îÄ train_tokenizer.py         # Entra√Ænement tokenizer BPE
‚îú‚îÄ‚îÄ analyze_dataset.py         # Analyse de donn√©es
‚îú‚îÄ‚îÄ requirements.txt           # D√©pendances Python
‚îú‚îÄ‚îÄ README.md                  # Ce fichier
‚îú‚îÄ‚îÄ TUNING_GUIDE.md            # Guide d'optimisation hyperparam√®tres
‚îî‚îÄ‚îÄ TRAINING_GUIDE.md          # Guide d√©taill√© d'entra√Ænement avanc√©
```

---

## üéì Principes de Design Inspir√©s du Cerveau

| Principe Neurobiologique | Impl√©mentation HIM |
|--------------------------|---------------------|
| **M√©moire de travail (Cortex pr√©frontal)** | DifferentiableMemory (16 slots) |
| **Dual-process (Syst√®me 1/2)** | Fast path (intuitif) / Slow path (analytique) |
| **Parole int√©rieure** | SharedReasoningModule (scratchpad mental) |
| **Hi√©rarchie corticale** | 10 couches avec traitement progressif |
| **Attention s√©lective** | GQA avec top-down modulation |
| **Consolidation m√©moire** | M√©moire persistante entre batches |

---

## üî¨ Inspirations √âtat-de-l'Art

| Mod√®le | Technique | Application dans HIM |
|--------|-----------|----------------------|
| **Llama-2** (Meta) | Grouped-Query Attention | 8 Q heads, 2 KV heads |
| **PaLM** (Google) | SwiGLU activation | FFN moderne |
| **Phi-1** (Microsoft) | Depth > Width | 10 couches √ó 160 dim |
| **Gemma** (Google) | Efficacit√© param√©trique | Optimisation vocab, GQA |
| **TinyBERT** | Compression agressive | R√©duction dimensionnelle |

---

## üìä R√©sultats Attendus

### Performance (sur WikiText-103)
- **Perplexit√© cible**: < 50 (comp√©titif avec mod√®les 10-20M params)
- **BERTScore**: > 0.75 (compr√©hension s√©mantique)
- **Distinct-2**: > 0.6 (diversit√© lexicale)

### Efficacit√©
- **Entra√Ænement**: < 24h sur GPU unique
- **Inf√©rence**: > 500 tokens/sec sur CPU
- **M√©moire**: < 500MB RAM apr√®s quantization
- **D√©ploiement**: Mobile, Raspberry Pi

### Capacit√©s Cognitives
- **M√©moire**: Peut tracker ~16 entit√©s/faits simultan√©ment
- **Raisonnement**: Raffinement en 3 √©tapes (layers 3, 6, 9)
- **Adaptation**: Routage dynamique selon complexit√© du token

---

## üõ†Ô∏è Optimisation Post-Entra√Ænement

### Pruning (20% par d√©faut)
```bash
python main.py --pruning_amount 0.2 --fine_tune_epochs 2
```
- Pruning non-structur√© al√©atoire
- Fine-tuning post-pruning
- R√©sultat: mod√®le plus compact sans perte significative

### Quantization (int8)
```bash
# Automatique en fin d'entra√Ænement
# R√©sultat: model_artifacts/nl_direct_response_model_quantized.pth
```
- Quantization dynamique
- R√©duction 4√ó m√©moire
- Inf√©rence CPU optimis√©e

---

## üìñ Documentation Compl√©mentaire

- **[TUNING_GUIDE.md](TUNING_GUIDE.md)**: Guide complet d'optimisation hyperparam√®tres
- **[TRAINING_GUIDE.md](TRAINING_GUIDE.md)**: Documentation d√©taill√©e de l'entra√Ænement avanc√©
- **Plan d'architecture**: `.claude/plans/recursive-jingling-teapot.md`

---

## ü§ù Contribution

Ce projet est un prototype de recherche. Les contributions sont bienvenues pour:
- Am√©liorer les m√©canismes cognitifs
- Tester sur d'autres datasets
- Optimiser l'architecture
- Ajouter de nouvelles m√©triques

---

## üìù Citation

```bibtex
@software{him2024,
  title={HIM: Hyper-Introspective Model},
  author={Votre Nom},
  year={2024},
  note={7M parameter cognitive language model}
}
```

---

## ‚öñÔ∏è Licence

MIT License - Voir LICENSE pour d√©tails

---

## üôè Remerciements

Inspir√© par:
- Google (Nested Learning, PaLM, Gemma)
- Meta (Llama-2)
- Microsoft (Phi-1)
- OpenAI (GPT architecture)
- DeepMind (Differentiable Neural Computer)

---

**HIM**: Proof that cognitive capabilities emerge from architectural design, not raw parameter count. üß†‚ú®
