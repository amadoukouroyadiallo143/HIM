{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# ðŸ§  HIM - Hyper-Introspective Model (7M Parameters)\n",
    "\n",
    "**Un modÃ¨le Transformer ultra-compact avec mÃ©canismes cognitifs avancÃ©s**\n",
    "\n",
    "- 7.16M paramÃ¨tres (99.66% de rÃ©duction vs 2.1B)\n",
    "- 4 mÃ©canismes cognitifs: MÃ©moire, Attention GQA, Multi-vitesse, Raisonnement\n",
    "- EntraÃ®nement sur WikiText-103\n",
    "- Architecture inspirÃ©e de Llama-2, PaLM, Phi-1\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‹ Instructions\n",
    "\n",
    "1. **Activer GPU**: Runtime â†’ Change runtime type â†’ GPU (T4)\n",
    "2. **ExÃ©cuter toutes les cellules** dans l'ordre\n",
    "3. **EntraÃ®nement**: ~6-8 heures pour 30 epochs sur GPU T4\n",
    "4. **TÃ©lÃ©charger le modÃ¨le** Ã  la fin\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-header"
   },
   "source": [
    "## 1ï¸âƒ£ Configuration et Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "check-gpu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'nvidia-smi' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# VÃ©rifier le GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "install-deps"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Installer les dÃ©pendances\n",
    "!pip install -q torch transformers datasets tokenizers tqdm evaluate sacrebleu bert-score nltk matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone-repo"
   },
   "outputs": [],
   "source": [
    "# Cloner le repository (ou uploader vos fichiers)\n",
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "# Option 1: Cloner depuis GitHub (si vous avez pushÃ© le code)\n",
    "# !git clone https://github.com/votre-username/HIM.git\n",
    "# %cd HIM\n",
    "\n",
    "# Option 2: Monter Google Drive et copier les fichiers\n",
    "drive.mount('/content/drive')\n",
    "print(\"\\nâš ï¸ IMPORTANT: Uploadez votre dossier HIM dans Google Drive\")\n",
    "print(\"Ensuite, dÃ©commentez et modifiez la ligne suivante:\")\n",
    "# !cp -r /content/drive/MyDrive/HIM /content/\n",
    "# %cd /content/HIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "manual-upload"
   },
   "outputs": [],
   "source": [
    "# Option 3: Upload manuel des fichiers (si pas de GitHub/Drive)\n",
    "# CrÃ©er la structure de dossiers\n",
    "!mkdir -p src model_artifacts/tokenizer\n",
    "\n",
    "print(\"ðŸ“ Uploadez maintenant les fichiers suivants:\")\n",
    "print(\"  src/components.py\")\n",
    "print(\"  src/model.py\")\n",
    "print(\"  src/dataset.py\")\n",
    "print(\"  src/train.py\")\n",
    "print(\"  src/optimizer.py\")\n",
    "print(\"  train_tokenizer.py\")\n",
    "print(\"  main.py\")\n",
    "print(\"\\nUtilisez le panneau de gauche (Files) pour uploader\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tokenizer-header"
   },
   "source": [
    "## 2ï¸âƒ£ EntraÃ®nement du Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train-tokenizer"
   },
   "outputs": [],
   "source": [
    "# EntraÃ®ner le tokenizer BPE (30K tokens)\n",
    "!python train_tokenizer.py --vocab_size 30000 --dataset_name wikitext --config_name wikitext-103-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "verify-tokenizer"
   },
   "outputs": [],
   "source": [
    "# VÃ©rifier le tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('model_artifacts/tokenizer')\n",
    "print(f\"âœ… Tokenizer chargÃ©: {len(tokenizer)} tokens\")\n",
    "\n",
    "# Test\n",
    "test_text = \"The quick brown fox jumps over the lazy dog. ðŸš€\"\n",
    "tokens = tokenizer.encode(test_text)\n",
    "decoded = tokenizer.decode(tokens)\n",
    "print(f\"\\nTest: '{test_text}'\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"DÃ©codÃ©: '{decoded}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training-header"
   },
   "source": [
    "## 3ï¸âƒ£ EntraÃ®nement du ModÃ¨le\n",
    "\n",
    "### Configuration recommandÃ©e pour GPU T4 (gratuit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training-config"
   },
   "outputs": [],
   "source": [
    "# Configuration d'entraÃ®nement\n",
    "CONFIG = {\n",
    "    # Architecture (7.16M params)\n",
    "    'embed_dim': 160,\n",
    "    'num_heads': 8,\n",
    "    'num_kv_heads': 2,  # GQA\n",
    "    'num_encoder_layers': 10,\n",
    "    'dim_feedforward': 320,\n",
    "    'mem_slots': 16,\n",
    "    'mem_rank': 16,\n",
    "    'reasoning_tokens': 64,\n",
    "    \n",
    "    # EntraÃ®nement\n",
    "    'num_epochs': 30,\n",
    "    'batch_size': 16,  # Plus Ã©levÃ© sur GPU\n",
    "    'accumulation_steps': 4,  # Batch effectif = 64\n",
    "    'learning_rate': 1e-3,\n",
    "    'grad_clip_value': 1.0,\n",
    "    'dropout': 0.1,\n",
    "    'label_smoothing': 0.1,\n",
    "    \n",
    "    # Optimisations\n",
    "    'fp16': True,  # Mixed precision\n",
    "    'use_cognitive_losses': True,\n",
    "    'smart_training': False,  # DÃ©sactivÃ© pour Colab\n",
    "    'save_steps': 5000,\n",
    "    \n",
    "    # Post-training\n",
    "    'pruning_amount': 0.2,\n",
    "    'fine_tune_epochs': 2\n",
    "}\n",
    "\n",
    "print(\"ðŸ“Š Configuration d'entraÃ®nement:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "start-training"
   },
   "outputs": [],
   "source": [
    "# Lancer l'entraÃ®nement\n",
    "!python main.py \\\n",
    "    --task_type wikitext \\\n",
    "    --embed_dim {CONFIG['embed_dim']} \\\n",
    "    --num_heads {CONFIG['num_heads']} \\\n",
    "    --num_kv_heads {CONFIG['num_kv_heads']} \\\n",
    "    --num_encoder_layers {CONFIG['num_encoder_layers']} \\\n",
    "    --dim_feedforward {CONFIG['dim_feedforward']} \\\n",
    "    --mem_slots {CONFIG['mem_slots']} \\\n",
    "    --mem_rank {CONFIG['mem_rank']} \\\n",
    "    --reasoning_tokens {CONFIG['reasoning_tokens']} \\\n",
    "    --num_epochs {CONFIG['num_epochs']} \\\n",
    "    --batch_size {CONFIG['batch_size']} \\\n",
    "    --accumulation_steps {CONFIG['accumulation_steps']} \\\n",
    "    --learning_rate {CONFIG['learning_rate']} \\\n",
    "    --grad_clip_value {CONFIG['grad_clip_value']} \\\n",
    "    --dropout {CONFIG['dropout']} \\\n",
    "    --label_smoothing {CONFIG['label_smoothing']} \\\n",
    "    --fp16 \\\n",
    "    --use_cognitive_losses \\\n",
    "    --pruning_amount {CONFIG['pruning_amount']} \\\n",
    "    --fine_tune_epochs {CONFIG['fine_tune_epochs']} \\\n",
    "    --save_steps {CONFIG['save_steps']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "monitoring-header"
   },
   "source": [
    "## 4ï¸âƒ£ Monitoring et Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot-metrics"
   },
   "outputs": [],
   "source": [
    "# Visualiser les mÃ©triques d'entraÃ®nement\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Charger les mÃ©triques\n",
    "with open('model_artifacts/training_metrics.json', 'r') as f:\n",
    "    metrics = json.load(f)\n",
    "\n",
    "# Plot perplexitÃ©\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# PerplexitÃ©\n",
    "axes[0, 0].plot(metrics['train_perplexity'], label='Train')\n",
    "axes[0, 0].plot(metrics['val_perplexity'], label='Validation')\n",
    "axes[0, 0].set_title('PerplexitÃ©')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('PerplexitÃ©')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Loss\n",
    "axes[0, 1].plot(metrics['train_loss'], label='Train')\n",
    "axes[0, 1].plot(metrics['val_loss'], label='Validation')\n",
    "axes[0, 1].set_title('Loss')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# BERTScore\n",
    "axes[1, 0].plot(metrics['val_bertscore'], label='BERTScore F1')\n",
    "axes[1, 0].set_title('BERTScore (QualitÃ© SÃ©mantique)')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Score')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Distinct-2\n",
    "axes[1, 1].plot(metrics['val_distinct_2'], label='Distinct-2')\n",
    "axes[1, 1].set_title('Distinct-2 (DiversitÃ©)')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Score')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Courbes d'entraÃ®nement sauvegardÃ©es dans 'training_curves.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cognitive-metrics"
   },
   "outputs": [],
   "source": [
    "# Visualiser les mÃ©triques cognitives\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "with open('model_artifacts/cognitive_metrics.json', 'r') as f:\n",
    "    cog_metrics = json.load(f)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Memory utilization\n",
    "axes[0].plot(cog_metrics['memory_utilization'])\n",
    "axes[0].set_title('Utilisation MÃ©moire (%)')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('% Slots Actifs')\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Reasoning diversity\n",
    "axes[1].plot(cog_metrics['reasoning_diversity'])\n",
    "axes[1].set_title('DiversitÃ© Raisonnement')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].grid(True)\n",
    "\n",
    "# Routing balance\n",
    "axes[2].plot(cog_metrics['avg_slow_preference'], label='Slow Path')\n",
    "axes[2].plot(cog_metrics['avg_fast_preference'], label='Fast Path')\n",
    "axes[2].set_title('Balance Routage')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('PrÃ©fÃ©rence (%)')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('cognitive_metrics.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… MÃ©triques cognitives sauvegardÃ©es dans 'cognitive_metrics.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inference-header"
   },
   "source": [
    "## 5ï¸âƒ£ Test d'InfÃ©rence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test-inference"
   },
   "outputs": [],
   "source": [
    "# Tester le modÃ¨le entraÃ®nÃ©\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from src.model import NLDirectResponse\n",
    "\n",
    "# Charger le modÃ¨le\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = AutoTokenizer.from_pretrained('model_artifacts/tokenizer')\n",
    "\n",
    "checkpoint = torch.load('model_artifacts/nl_direct_response_model_best.pth', map_location=device)\n",
    "config = checkpoint['config']\n",
    "\n",
    "model = NLDirectResponse(\n",
    "    vocab_size=config['vocab_size'],\n",
    "    embed_dim=config['embed_dim'],\n",
    "    num_heads=config['num_heads'],\n",
    "    num_kv_heads=config['num_kv_heads'],\n",
    "    num_encoder_layers=config['num_encoder_layers'],\n",
    "    dim_feedforward=config['dim_feedforward'],\n",
    "    max_answer_len=config['max_answer_len'],\n",
    "    dropout=0.0,\n",
    "    mem_slots=config['mem_slots'],\n",
    "    mem_rank=config['mem_rank'],\n",
    "    reasoning_tokens=config['reasoning_tokens']\n",
    ")\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"âœ… ModÃ¨le chargÃ© avec succÃ¨s!\")\n",
    "print(f\"Epoch: {checkpoint.get('epoch', 'N/A')}\")\n",
    "print(f\"Best Val Loss: {checkpoint.get('best_val_loss', 'N/A'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate-text"
   },
   "outputs": [],
   "source": [
    "# GÃ©nÃ©rer du texte\n",
    "def generate_text(prompt, temperature=0.8, top_k=50, top_p=0.9):\n",
    "    model.reset_memory()\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors='pt', padding='max_length',\n",
    "                      max_length=512, truncation=True)\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, src_padding_mask=attention_mask)\n",
    "    \n",
    "    # Sampling avec tempÃ©rature\n",
    "    logits = logits[0] / temperature\n",
    "    \n",
    "    # Top-k filtering\n",
    "    if top_k > 0:\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k, dim=-1)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = float('-inf')\n",
    "    \n",
    "    # Ã‰chantillonnage\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    predicted_ids = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
    "    \n",
    "    generated_text = tokenizer.decode(predicted_ids, skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Tests\n",
    "test_prompts = [\n",
    "    \"The history of artificial intelligence began in\",\n",
    "    \"Machine learning is a field of study that\",\n",
    "    \"In the future, technology will\",\n",
    "    \"The most important discovery in science was\"\n",
    "]\n",
    "\n",
    "print(\"ðŸŽ¨ GÃ©nÃ©ration de texte:\\n\")\n",
    "for prompt in test_prompts:\n",
    "    generated = generate_text(prompt)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"GÃ©nÃ©rÃ©: {generated}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download-header"
   },
   "source": [
    "## 6ï¸âƒ£ TÃ©lÃ©chargement des RÃ©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zip-results"
   },
   "outputs": [],
   "source": [
    "# Compresser tous les artefacts\n",
    "!zip -r HIM_trained_model.zip model_artifacts/ *.png\n",
    "\n",
    "print(\"âœ… Fichiers compressÃ©s dans HIM_trained_model.zip\")\n",
    "print(\"\\nContenu:\")\n",
    "!unzip -l HIM_trained_model.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download-files"
   },
   "outputs": [],
   "source": [
    "# TÃ©lÃ©charger le fichier\n",
    "from google.colab import files\n",
    "\n",
    "files.download('HIM_trained_model.zip')\n",
    "print(\"ðŸ“¥ TÃ©lÃ©chargement dÃ©marrÃ©!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save-to-drive"
   },
   "outputs": [],
   "source": [
    "# Ou sauvegarder dans Google Drive\n",
    "!cp HIM_trained_model.zip /content/drive/MyDrive/\n",
    "print(\"âœ… ModÃ¨le sauvegardÃ© dans Google Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary-header"
   },
   "source": [
    "## ðŸ“Š RÃ©sumÃ© Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "final-summary"
   },
   "outputs": [],
   "source": [
    "# Afficher le rÃ©sumÃ© final\n",
    "import json\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"HIM - RÃ‰SUMÃ‰ DE L'ENTRAÃŽNEMENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Charger les mÃ©triques finales\n",
    "with open('model_artifacts/training_metrics.json', 'r') as f:\n",
    "    metrics = json.load(f)\n",
    "\n",
    "print(f\"\\nðŸ“ˆ MÃ©triques Finales:\")\n",
    "print(f\"  Epochs entraÃ®nÃ©s: {len(metrics['train_loss'])}\")\n",
    "print(f\"  Best Val Loss: {min(metrics['val_loss']):.4f}\")\n",
    "print(f\"  Best Perplexity: {min(metrics['val_perplexity']):.2f}\")\n",
    "print(f\"  Best BERTScore: {max(metrics['val_bertscore']):.4f}\")\n",
    "print(f\"  Best Distinct-2: {max(metrics['val_distinct_2']):.4f}\")\n",
    "\n",
    "with open('model_artifacts/cognitive_metrics.json', 'r') as f:\n",
    "    cog_metrics = json.load(f)\n",
    "\n",
    "print(f\"\\nðŸ§  MÃ©canismes Cognitifs:\")\n",
    "print(f\"  Utilisation mÃ©moire finale: {cog_metrics['memory_utilization'][-1]:.1f}%\")\n",
    "print(f\"  DiversitÃ© raisonnement: {cog_metrics['reasoning_diversity'][-1]:.4f}\")\n",
    "print(f\"  Balance routage: Slow={cog_metrics['avg_slow_preference'][-1]:.1f}%, Fast={cog_metrics['avg_fast_preference'][-1]:.1f}%\")\n",
    "\n",
    "# ParamÃ¨tres du modÃ¨le\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nâš™ï¸ Architecture:\")\n",
    "print(f\"  ParamÃ¨tres totaux: {total_params:,}\")\n",
    "print(f\"  Taille du modÃ¨le: ~{total_params * 4 / 1024 / 1024:.1f} MB (float32)\")\n",
    "print(f\"  Taille quantisÃ©e: ~{total_params / 1024 / 1024:.1f} MB (int8)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… ENTRAÃŽNEMENT TERMINÃ‰ AVEC SUCCÃˆS!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nFichiers gÃ©nÃ©rÃ©s:\")\n",
    "print(\"  - model_artifacts/nl_direct_response_model_best.pth (modÃ¨le)\")\n",
    "print(\"  - model_artifacts/nl_direct_response_model_quantized.pth (quantisÃ©)\")\n",
    "print(\"  - model_artifacts/training_metrics.json (mÃ©triques)\")\n",
    "print(\"  - training_curves.png (visualisations)\")\n",
    "print(\"  - cognitive_metrics.png (mÃ©canismes cognitifs)\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
